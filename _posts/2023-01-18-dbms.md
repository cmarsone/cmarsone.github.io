---
layout: post
title: Database Management System
---

*Databases* (DB) : Organized collection of data. This collection must have a regular structure,
the data is meant to capture a subset of the "real" world; the collections
are connected to each other and deal with a same subject. The data are
organized in a way that facilitates their manipulation (access, updates).

*Database Management System* (DBMS) : Dedicated software to manage databases. Organizes data storage, handles
access to data: queries, updates...

[DBMS Ranking](https://db-engines.com/en/ranking_trend)

What DBMS offers :
1. Persistent storage of data in files: 
2. Performance for queries index, query optimization...
3. Multi-user (concurrent access) transactions, locks, MVCC. . .
4. Security (access control) access rights. . .
5. Easy to use (queries, maintenance, evolution)
6. Reliability as a system (recovery after failure. . . ) logs, backups. . .
7. Data integrity (consistency)

Alternative NoSQL solutions (or plain files) may be a better choice when:
- you have too much data: > 1TB
- you need very low latency
- tiny IOT devices with limited memory
- the relational model does not fit your data (graph, time series) and queries
- your software stack integrates better with other data storage technology

*Business intelligence* (BI): set of techniques and tools that enable a
company to transform business data into into meaningful and useful
information for decision making

*DataWarehouse* (DW): repository that stores the data and infrastructure to
support analysis.

*Data relational model* : data represented by tables
*SQL* : declarative language to query & update data in relational model
*Schema* : describes data organization, the format of each column
vs *Instance*: the actual data in DB that must be organized according to the schema

3 abstraction levels in a DMBS : external (access), logical (schema), physical (storage)

DB Lifecycle : modelization and specifications / schema design, initialization / manipulations, queries, updates / maintenance (optimizations, patches, evolutions)

### Relational DBMS $\to$ Distributed DBMS

*Why* ? : parallelism, scalability, accesibility & fault-tolerance, optimization for different hardware, distribute geographically...
*How* ? : sharding ($\sim$ horizontal partitioning), but also replication

*Parallelism* : between CPUs, in the CPUs, in the queries $\to$ Large-Scale data distributed, shared memory

*Replication* : reliability, read performance by synchronization, versioning, consensus

*Challenges when distributing* : good data partition/replication, coherence (trade-off between performance and integrity when dealing with reads and writes), distributing computation tasks, fault tolerance, transaction control, data privacy

3 guarantees (CAP)
- *Consistency*: Every read request to the system receives data corresponding to the most recent read
- *Availability*: The system must answer any query to the system, even if the answer is wrong or outdated.
- *Partition tolerance*: The system must answer queries even under arbitrary failures of distributed nodes or of messages between nodes.

$\to$ *CAP Theorem* : A system cannot have all 3. When data is partitioned, we cannot guarantee both availability and consistency.
examples : ACID mode $\to$ consistency / BASE model $\to$ availability

#### NoSQL

Data model is not fixed but *autonomous* (denormalization, no joins). NoSQL is focused on data exchange & distribution
Disk seek : 10ms

#### Hadoop

- *HDFS* : distributed file system with high-throughput, reliable access to data
- *MapReduce* : model to process more data in parallel
- *YARN* (Yet Another Resource Navigator) : framework for job scheduling and cluster resource
- *Ozone* : object store for Hadoop

*Main idea* : distributed processing of large datasets accross clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines. Also managing failures in the framework instead of at hardware level

A distributed file system:
- designed to manage huge data (TB, PB)
- while optimizing sequential reads => No random reads within a block => designed to optimize throughput forsequential reads, not latency.
- immutable files (support append, truncate): Write-once Read-many (CREW model ?)
- distributed on “commodity hardware” clusters opposed to higher-end servers with built-in parallelization
- need for fault-tolerance $\rightarrow$ replication

[MapReduce Patterns](https://highlyscalable.wordpress.com/2012/02/01/mapreduce-patterns/)

[HDFS Architecture](https://hadoop.apache.org/docs/r3.0.1/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)

![image](https://user-images.githubusercontent.com/109908559/216018329-8e7c3130-1541-4093-b720-2dfde0dc6ce5.png)

Master-slave architecture:
- *NameNode*: (1) manages filesystem (tree+metadata) on disk+RAM, replicated 
              (2) manages block placement, creation, replication in memory. Rebuilt if needed.
               Datanodes are common storage for all NameNodes they may contain blocks from several namespace.
- *DataNode*: contains blocks of data. Read from disk in general, but can be cached in memory.

HDFS splits data into large blocks. When HDFS splits input file, each chunk= 1 HDFS block. Blocks distributed among datanodes. HDFS placement is rack-aware. To
optimize placement of copies and queries and to optimize queries by using data locality (same node<rack<datacenter).
Map writes output on local disk, not HDFS (no replica), Reduce on HDFS.

A *hash tree* or *Merkle tree* is a tree in which every "leaf" (node) is labelled with the cryptographic hash (MD5) of a data block
